{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SRCNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1bqreBXSd2XG9JE7u5wEIl1wG4dVQxsV0","authorship_tag":"ABX9TyMfn6tf8c/uOI/vD54NI7Hh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":663},"id":"ncbP11Xmg2le","executionInfo":{"status":"ok","timestamp":1621787852270,"user_tz":-480,"elapsed":129435,"user":{"displayName":"YUFEI YANG","photoUrl":"","userId":"02489604091887462027"}},"outputId":"b305abc7-bbdd-4928-a973-66d00ce38636"},"source":["!pip install torch==1.0.0 torchvision==0.2.1\n","!pip install Pillow==6.2.2"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting torch==1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/3b/0b8de6e654c2983898564226792c6f09d9bcaba97b7b29c40e4ed4ae43ed/torch-1.0.0-cp37-cp37m-manylinux1_x86_64.whl (591.8MB)\n","\u001b[K     |████████████████████████████████| 591.8MB 28kB/s \n","\u001b[?25hCollecting torchvision==0.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.2.1) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.2.1) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.2.1) (7.1.2)\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.0.0 which is incompatible.\u001b[0m\n","Installing collected packages: torch, torchvision\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","  Found existing installation: torchvision 0.9.1+cu101\n","    Uninstalling torchvision-0.9.1+cu101:\n","      Successfully uninstalled torchvision-0.9.1+cu101\n","Successfully installed torch-1.0.0 torchvision-0.2.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting Pillow==6.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/3f/03375124676ab49ca6e6917c0f1f663afb8354d5d24e12f4fe4587a39ae2/Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 11.1MB/s \n","\u001b[31mERROR: bokeh 2.3.2 has requirement pillow>=7.1.0, but you'll have pillow 6.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: Pillow\n","  Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","Successfully installed Pillow-6.2.2\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ntkz0MCg-Mo","executionInfo":{"status":"ok","timestamp":1621787892241,"user_tz":-480,"elapsed":397,"user":{"displayName":"YUFEI YANG","photoUrl":"","userId":"02489604091887462027"}},"outputId":"2b472f38-bcc1-41c0-efda-05e04f0d4b4b"},"source":["import torch\n","print(torch.__version__)\n","print(torch.cuda.is_available())"],"execution_count":1,"outputs":[{"output_type":"stream","text":["1.0.0\n","True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdjYmKMM7ceG","executionInfo":{"status":"ok","timestamp":1621787898154,"user_tz":-480,"elapsed":3759,"user":{"displayName":"YUFEI YANG","photoUrl":"","userId":"02489604091887462027"}},"outputId":"64dbe1bb-e005-4553-ed3c-5d530aca90a9"},"source":["!python3 -m pip install SSIM-PIL"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting SSIM-PIL\n","  Downloading https://files.pythonhosted.org/packages/98/a9/8bc66558b06d99862aa84f85995f1ded8ff38fcd116467f9c371ba2ba51b/SSIM_PIL-1.0.13-py3-none-any.whl\n","Installing collected packages: SSIM-PIL\n","Successfully installed SSIM-PIL-1.0.13\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnVc25wLXDKL","executionInfo":{"status":"ok","timestamp":1621788156162,"user_tz":-480,"elapsed":253248,"user":{"displayName":"YUFEI YANG","photoUrl":"","userId":"02489604091887462027"}},"outputId":"8bbdf017-685f-4f35-dcde-414480e0ee93"},"source":["# https://github.com/yjn870/SRCNN-pytorch\n","import argparse\n","import glob\n","import h5py\n","import numpy as np\n","import PIL.Image as pil_image\n","from SSIM_PIL import compare_ssim\n","import torch\n","import numpy as np\n","\n","# utils\n","def convert_rgb_to_y(img):\n","    if type(img) == np.ndarray:\n","        return 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) / 256.\n","    elif type(img) == torch.Tensor:\n","        if len(img.shape) == 4:\n","            img = img.squeeze(0)\n","        return 16. + (64.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) / 256.\n","    else:\n","        raise Exception('Unknown Type', type(img))\n","\n","\n","def convert_rgb_to_ycbcr(img):\n","    if type(img) == np.ndarray:\n","        y = 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) / 256.\n","        cb = 128. + (-37.945 * img[:, :, 0] - 74.494 * img[:, :, 1] + 112.439 * img[:, :, 2]) / 256.\n","        cr = 128. + (112.439 * img[:, :, 0] - 94.154 * img[:, :, 1] - 18.285 * img[:, :, 2]) / 256.\n","        return np.array([y, cb, cr]).transpose([1, 2, 0])\n","    elif type(img) == torch.Tensor:\n","        if len(img.shape) == 4:\n","            img = img.squeeze(0)\n","        y = 16. + (64.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) / 256.\n","        cb = 128. + (-37.945 * img[0, :, :] - 74.494 * img[1, :, :] + 112.439 * img[2, :, :]) / 256.\n","        cr = 128. + (112.439 * img[0, :, :] - 94.154 * img[1, :, :] - 18.285 * img[2, :, :]) / 256.\n","        return torch.cat([y, cb, cr], 0).permute(1, 2, 0)\n","    else:\n","        raise Exception('Unknown Type', type(img))\n","\n","\n","def convert_ycbcr_to_rgb(img):\n","    if type(img) == np.ndarray:\n","        r = 298.082 * img[:, :, 0] / 256. + 408.583 * img[:, :, 2] / 256. - 222.921\n","        g = 298.082 * img[:, :, 0] / 256. - 100.291 * img[:, :, 1] / 256. - 208.120 * img[:, :, 2] / 256. + 135.576\n","        b = 298.082 * img[:, :, 0] / 256. + 516.412 * img[:, :, 1] / 256. - 276.836\n","        return np.array([r, g, b]).transpose([1, 2, 0])\n","    elif type(img) == torch.Tensor:\n","        if len(img.shape) == 4:\n","            img = img.squeeze(0)\n","        r = 298.082 * img[0, :, :] / 256. + 408.583 * img[2, :, :] / 256. - 222.921\n","        g = 298.082 * img[0, :, :] / 256. - 100.291 * img[1, :, :] / 256. - 208.120 * img[2, :, :] / 256. + 135.576\n","        b = 298.082 * img[0, :, :] / 256. + 516.412 * img[1, :, :] / 256. - 276.836\n","        return torch.cat([r, g, b], 0).permute(1, 2, 0)\n","    else:\n","        raise Exception('Unknown Type', type(img))\n","\n","\n","def calc_psnr(img1, img2):\n","    return 10. * torch.log10(1. / torch.mean((img1 - img2) ** 2))\n","\n","def calc_ssim(img1, img2):\n","    return compare_ssim(img1, img2)\n","\n","\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","# generate dataset\n","def trainset_generate(images_dir, output_path, scale, stride=56, patch_size=66):\n","    h5_file = h5py.File(output_path, 'w')\n","\n","    lr_patches = []\n","    hr_patches = []\n","    index = 1\n","\n","    for image_path in sorted(glob.glob('{}/*'.format(images_dir))):\n","        print('process {0} image'.format(index))\n","        hr = pil_image.open(image_path).convert('RGB')\n","        hr_width = (hr.width // scale) * scale\n","        hr_height = (hr.height // scale) * scale\n","        hr = hr.resize((hr_width, hr_height), resample=pil_image.BICUBIC)\n","        lr = hr.resize((hr_width // scale, hr_height // scale), resample=pil_image.BICUBIC)\n","        lr = lr.resize((lr.width * scale, lr.height * scale), resample=pil_image.BICUBIC)\n","        hr = np.array(hr).astype(np.float32)\n","        lr = np.array(lr).astype(np.float32)\n","        hr = convert_rgb_to_y(hr)\n","        lr = convert_rgb_to_y(lr)\n","\n","        for i in range(0, lr.shape[0] - patch_size + 1, stride):\n","            for j in range(0, lr.shape[1] - patch_size + 1, stride):\n","                lr_patches.append(lr[i:i + patch_size, j:j + patch_size])\n","                hr_patches.append(hr[i:i + patch_size, j:j + patch_size])\n","        index = index + 1\n","\n","    lr_patches = np.array(lr_patches)\n","    hr_patches = np.array(hr_patches)\n","\n","    h5_file.create_dataset('lr', data=lr_patches)\n","    h5_file.create_dataset('hr', data=hr_patches)\n","\n","    h5_file.close()\n","\n","\n","# generate training dataset\n","scale = 2\n","trainset_generate('/content/drive/MyDrive/CS17 project/data/HR/', '/content/drive/MyDrive/CS17 project/data/train.hdf5', scale=scale)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["process 1 image\n","process 2 image\n","process 3 image\n","process 4 image\n","process 5 image\n","process 6 image\n","process 7 image\n","process 8 image\n","process 9 image\n","process 10 image\n","process 11 image\n","process 12 image\n","process 13 image\n","process 14 image\n","process 15 image\n","process 16 image\n","process 17 image\n","process 18 image\n","process 19 image\n","process 20 image\n","process 21 image\n","process 22 image\n","process 23 image\n","process 24 image\n","process 25 image\n","process 26 image\n","process 27 image\n","process 28 image\n","process 29 image\n","process 30 image\n","process 31 image\n","process 32 image\n","process 33 image\n","process 34 image\n","process 35 image\n","process 36 image\n","process 37 image\n","process 38 image\n","process 39 image\n","process 40 image\n","process 41 image\n","process 42 image\n","process 43 image\n","process 44 image\n","process 45 image\n","process 46 image\n","process 47 image\n","process 48 image\n","process 49 image\n","process 50 image\n","process 51 image\n","process 52 image\n","process 53 image\n","process 54 image\n","process 55 image\n","process 56 image\n","process 57 image\n","process 58 image\n","process 59 image\n","process 60 image\n","process 61 image\n","process 62 image\n","process 63 image\n","process 64 image\n","process 65 image\n","process 66 image\n","process 67 image\n","process 68 image\n","process 69 image\n","process 70 image\n","process 71 image\n","process 72 image\n","process 73 image\n","process 74 image\n","process 75 image\n","process 76 image\n","process 77 image\n","process 78 image\n","process 79 image\n","process 80 image\n","process 81 image\n","process 82 image\n","process 83 image\n","process 84 image\n","process 85 image\n","process 86 image\n","process 87 image\n","process 88 image\n","process 89 image\n","process 90 image\n","process 91 image\n","process 92 image\n","process 93 image\n","process 94 image\n","process 95 image\n","process 96 image\n","process 97 image\n","process 98 image\n","process 99 image\n","process 100 image\n","process 101 image\n","process 102 image\n","process 103 image\n","process 104 image\n","process 105 image\n","process 106 image\n","process 107 image\n","process 108 image\n","process 109 image\n","process 110 image\n","process 111 image\n","process 112 image\n","process 113 image\n","process 114 image\n","process 115 image\n","process 116 image\n","process 117 image\n","process 118 image\n","process 119 image\n","process 120 image\n","process 121 image\n","process 122 image\n","process 123 image\n","process 124 image\n","process 125 image\n","process 126 image\n","process 127 image\n","process 128 image\n","process 129 image\n","process 130 image\n","process 131 image\n","process 132 image\n","process 133 image\n","process 134 image\n","process 135 image\n","process 136 image\n","process 137 image\n","process 138 image\n","process 139 image\n","process 140 image\n","process 141 image\n","process 142 image\n","process 143 image\n","process 144 image\n","process 145 image\n","process 146 image\n","process 147 image\n","process 148 image\n","process 149 image\n","process 150 image\n","process 151 image\n","process 152 image\n","process 153 image\n","process 154 image\n","process 155 image\n","process 156 image\n","process 157 image\n","process 158 image\n","process 159 image\n","process 160 image\n","process 161 image\n","process 162 image\n","process 163 image\n","process 164 image\n","process 165 image\n","process 166 image\n","process 167 image\n","process 168 image\n","process 169 image\n","process 170 image\n","process 171 image\n","process 172 image\n","process 173 image\n","process 174 image\n","process 175 image\n","process 176 image\n","process 177 image\n","process 178 image\n","process 179 image\n","process 180 image\n","process 181 image\n","process 182 image\n","process 183 image\n","process 184 image\n","process 185 image\n","process 186 image\n","process 187 image\n","process 188 image\n","process 189 image\n","process 190 image\n","process 191 image\n","process 192 image\n","process 193 image\n","process 194 image\n","process 195 image\n","process 196 image\n","process 197 image\n","process 198 image\n","process 199 image\n","process 200 image\n","process 201 image\n","process 202 image\n","process 203 image\n","process 204 image\n","process 205 image\n","process 206 image\n","process 207 image\n","process 208 image\n","process 209 image\n","process 210 image\n","process 211 image\n","process 212 image\n","process 213 image\n","process 214 image\n","process 215 image\n","process 216 image\n","process 217 image\n","process 218 image\n","process 219 image\n","process 220 image\n","process 221 image\n","process 222 image\n","process 223 image\n","process 224 image\n","process 225 image\n","process 226 image\n","process 227 image\n","process 228 image\n","process 229 image\n","process 230 image\n","process 231 image\n","process 232 image\n","process 233 image\n","process 234 image\n","process 235 image\n","process 236 image\n","process 237 image\n","process 238 image\n","process 239 image\n","process 240 image\n","process 241 image\n","process 242 image\n","process 243 image\n","process 244 image\n","process 245 image\n","process 246 image\n","process 247 image\n","process 248 image\n","process 249 image\n","process 250 image\n","process 251 image\n","process 252 image\n","process 253 image\n","process 254 image\n","process 255 image\n","process 256 image\n","process 257 image\n","process 258 image\n","process 259 image\n","process 260 image\n","process 261 image\n","process 262 image\n","process 263 image\n","process 264 image\n","process 265 image\n","process 266 image\n","process 267 image\n","process 268 image\n","process 269 image\n","process 270 image\n","process 271 image\n","process 272 image\n","process 273 image\n","process 274 image\n","process 275 image\n","process 276 image\n","process 277 image\n","process 278 image\n","process 279 image\n","process 280 image\n","process 281 image\n","process 282 image\n","process 283 image\n","process 284 image\n","process 285 image\n","process 286 image\n","process 287 image\n","process 288 image\n","process 289 image\n","process 290 image\n","process 291 image\n","process 292 image\n","process 293 image\n","process 294 image\n","process 295 image\n","process 296 image\n","process 297 image\n","process 298 image\n","process 299 image\n","process 300 image\n","process 301 image\n","process 302 image\n","process 303 image\n","process 304 image\n","process 305 image\n","process 306 image\n","process 307 image\n","process 308 image\n","process 309 image\n","process 310 image\n","process 311 image\n","process 312 image\n","process 313 image\n","process 314 image\n","process 315 image\n","process 316 image\n","process 317 image\n","process 318 image\n","process 319 image\n","process 320 image\n","process 321 image\n","process 322 image\n","process 323 image\n","process 324 image\n","process 325 image\n","process 326 image\n","process 327 image\n","process 328 image\n","process 329 image\n","process 330 image\n","process 331 image\n","process 332 image\n","process 333 image\n","process 334 image\n","process 335 image\n","process 336 image\n","process 337 image\n","process 338 image\n","process 339 image\n","process 340 image\n","process 341 image\n","process 342 image\n","process 343 image\n","process 344 image\n","process 345 image\n","process 346 image\n","process 347 image\n","process 348 image\n","process 349 image\n","process 350 image\n","process 351 image\n","process 352 image\n","process 353 image\n","process 354 image\n","process 355 image\n","process 356 image\n","process 357 image\n","process 358 image\n","process 359 image\n","process 360 image\n","process 361 image\n","process 362 image\n","process 363 image\n","process 364 image\n","process 365 image\n","process 366 image\n","process 367 image\n","process 368 image\n","process 369 image\n","process 370 image\n","process 371 image\n","process 372 image\n","process 373 image\n","process 374 image\n","process 375 image\n","process 376 image\n","process 377 image\n","process 378 image\n","process 379 image\n","process 380 image\n","process 381 image\n","process 382 image\n","process 383 image\n","process 384 image\n","process 385 image\n","process 386 image\n","process 387 image\n","process 388 image\n","process 389 image\n","process 390 image\n","process 391 image\n","process 392 image\n","process 393 image\n","process 394 image\n","process 395 image\n","process 396 image\n","process 397 image\n","process 398 image\n","process 399 image\n","process 400 image\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvHbjAYT2t3W","executionInfo":{"status":"ok","timestamp":1621789109652,"user_tz":-480,"elapsed":929024,"user":{"displayName":"YUFEI YANG","photoUrl":"","userId":"02489604091887462027"}},"outputId":"d81ac17b-4091-4576-c380-d637f1e254c6"},"source":["from torch import nn\n","import h5py\n","import numpy as np\n","from torch.utils.data import Dataset\n","import torch.optim as optim\n","from torch.utils.data.dataloader import DataLoader\n","from tqdm import tqdm\n","\n","class SRCNN(nn.Module):\n","    def __init__(self, num_channels=1):\n","        super(SRCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)\n","        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)\n","        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=5 // 2)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.conv3(x)\n","        return x\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, h5_file):\n","        super(TrainDataset, self).__init__()\n","        self.h5_file = h5_file\n","\n","    def __getitem__(self, idx):\n","        with h5py.File(self.h5_file, 'r') as f:\n","            return np.expand_dims(f['lr'][idx] / 255., 0), np.expand_dims(f['hr'][idx] / 255., 0)\n","\n","    def __len__(self):\n","        with h5py.File(self.h5_file, 'r') as f:\n","            return len(f['lr'])\n","\n","\n","# hyperparameters\n","lr = 1e-4\n","train_file = \"/content/drive/MyDrive/CS17 project/data/train.hdf5\"\n","batch_size = 256\n","num_workers = 2\n","num_epochs = 5 \n","\n","model = SRCNN().cuda()\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam([\n","    {'params': model.conv1.parameters()},\n","    {'params': model.conv2.parameters()},\n","    {'params': model.conv3.parameters(), 'lr': lr * 0.1}\n","], lr=lr)\n","\n","train_dataset = TrainDataset(train_file)\n","train_dataloader = DataLoader(dataset=train_dataset,\n","                              batch_size=batch_size,\n","                              shuffle=True,\n","                              num_workers=num_workers,\n","                              pin_memory=True,\n","                              drop_last=True)\n","\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_losses = AverageMeter()\n","\n","    with tqdm(total=(len(train_dataset) - len(train_dataset) % batch_size)) as t:\n","        t.set_description('epoch: {}/{}'.format(epoch, num_epochs - 1))\n","\n","        for data in train_dataloader:\n","            inputs, labels = data\n","\n","            inputs = inputs.cuda()\n","            labels = labels.cuda()\n","\n","            preds = model(inputs)\n","\n","            loss = criterion(preds, labels)\n","\n","            epoch_losses.update(loss.item(), len(inputs))\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n","            t.update(len(inputs))\n","\n","    model.eval()\n","    epoch_psnr = AverageMeter()\n","    epoch_ssim = AverageMeter()\n","\n","    for data in train_dataloader:\n","        inputs, labels = data\n","\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        with torch.no_grad():\n","            preds = model(inputs).clamp(0.0, 1.0)\n","\n","        epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n","\n","    print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["epoch: 0/4: 100%|██████████| 39936/39936 [01:36<00:00, 413.08it/s, loss=0.009370]\n","epoch: 1/4:   0%|          | 0/39936 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["eval psnr: 29.83\n"],"name":"stdout"},{"output_type":"stream","text":["epoch: 1/4: 100%|██████████| 39936/39936 [01:37<00:00, 410.07it/s, loss=0.000790]\n","epoch: 2/4:   0%|          | 0/39936 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["eval psnr: 31.86\n"],"name":"stdout"},{"output_type":"stream","text":["epoch: 2/4: 100%|██████████| 39936/39936 [01:34<00:00, 420.99it/s, loss=0.000607]\n","epoch: 3/4:   0%|          | 0/39936 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["eval psnr: 32.46\n"],"name":"stdout"},{"output_type":"stream","text":["epoch: 3/4: 100%|██████████| 39936/39936 [01:33<00:00, 426.80it/s, loss=0.000548]\n","epoch: 4/4:   0%|          | 0/39936 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["eval psnr: 32.79\n"],"name":"stdout"},{"output_type":"stream","text":["epoch: 4/4: 100%|██████████| 39936/39936 [01:35<00:00, 418.93it/s, loss=0.000515]\n"],"name":"stderr"},{"output_type":"stream","text":["eval psnr: 33.02\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"hnP0rV5GIyaC","executionInfo":{"status":"error","timestamp":1621789826593,"user_tz":-480,"elapsed":429,"user":{"displayName":"YUFEI YANG","photoUrl":"","userId":"02489604091887462027"}},"outputId":"30b50b66-fa8b-4af5-a476-bdce9f577a48"},"source":["import os\n","from PIL import Image\n","from torchvision import transforms\n","\n","img_dir = \"/content/drive/MyDrive/CS17 project/data/HR/\"\n","\n","ssim = 0\n","listdir=os.listdir(img_dir)\n","trans = transforms.ToTensor()\n","for name in listdir:\n","  image_gt = Image.open(img_dir+name)\n","  image = image_gt.resize((image_gt.width // scale, image_gt.height // scale), resample=pil_image.BICUBIC)\n","  blur_image = image.resize((image.width * scale, image.height * scale), resample=pil_image.BICUBIC)\n","  reconstructed_img = model(trans(blur_image).cuda()).clamp(0.0, 1.0)\n","  ssim = ssim + compare_ssim(image_gt, reconstructed_img)\n","print('Average SSIM: {:.2f}'.format(ssim/len(files)))\n","\n"],"execution_count":18,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-fff41b5c3643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_gt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_gt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_gt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mblur_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mreconstructed_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblur_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcompare_ssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average SSIM: {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-bc3215e923a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 1, 9, 9], but got 3-dimensional input of size [3, 600, 600] instead"]}]},{"cell_type":"code","metadata":{"id":"h54c4ioCKkXl"},"source":[""],"execution_count":null,"outputs":[]}]}